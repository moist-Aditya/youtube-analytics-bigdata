version: '3.8'

services:
  hadoop-master:
    build: .
    container_name: hadoop-master
    environment:
      - HADOOP_ROLE=master
    volumes:
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
      - ../../data/raw:/data/raw
      - ../../backend/processed-data:/data/processed
      - hadoop_namenode:/hadoop_data/hdfs/namenode
      - hadoop_datanode:/hadoop_data/hdfs/datanode
    ports:
      - "9870:9870" # namenode web UI
      - "9000:9000" # HDFS
      - "8088:8088" # YARN RM
    networks:
      - hadoop-net

  hadoop-worker-1:
    build: .
    container_name: hadoop-worker-1
    environment:
      - HADOOP_ROLE=worker
    volumes:
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
      - ../../data/raw:/data/raw
      - ../../backend/processed-data:/data/processed
      - hadoop_datanode:/hadoop_data/hdfs/datanode
    networks:
      - hadoop-net

  hadoop-worker-2:
    build: .
    container_name: hadoop-worker-2
    environment:
      - HADOOP_ROLE=worker
    volumes:
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
      - ../../data/raw:/data/raw
      - ../../backend/processed-data:/data/processed
      - hadoop_datanode:/hadoop_data/hdfs/datanode
    networks:
      - hadoop-net

  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
      - PYSPARK_PYTHON=python3
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
    ports:
      - "8081:8080" # Spark master web UI remapped
      - "7077:7077"
    volumes:
      - ../../:/app
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
      - ../../backend/processed-data:/data/processed
      - ../../data/raw:/data/raw
    depends_on:
      - hadoop-master
    networks:
      - hadoop-net

  spark-worker-1:
    image: apache/spark:3.5.0
    container_name: spark-worker-1
    depends_on:
      - spark-master
      - hadoop-master
    environment:
      - SPARK_NO_DAEMONIZE=true
      - PYSPARK_PYTHON=python3
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    volumes:
      - ../../:/app
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
      - ../../backend/processed-data:/data/processed
      - ../../data/raw:/data/raw
    networks:
      - hadoop-net

  spark-worker-2:
    image: apache/spark:3.5.0
    container_name: spark-worker-2
    depends_on:
      - spark-master
      - hadoop-master
    environment:
      - SPARK_NO_DAEMONIZE=true
      - PYSPARK_PYTHON=python3
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    volumes:
      - ../../:/app
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
      - ../../backend/processed-data:/data/processed
      - ../../data/raw:/data/raw
    networks:
      - hadoop-net

  fastapi:
    build: ../../backend
    container_name: fastapi
    volumes:
      - ../../backend/fastapi:/app
      - ../../backend/processed-data:/data/processed
      - /var/run/docker.sock:/var/run/docker.sock
      - ./hadoop_conf:/opt/hadoop/etc/hadoop
    environment:
      - PROCESSED_DIR=/data/processed
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    ports:
      - "8000:8000"
    depends_on:
      - hadoop-master
      - spark-master
    networks:
      - hadoop-net

networks:
  hadoop-net:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
